{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Image and Tabular Data in a predictive model\n",
    "\n",
    "Revisiting a clasic idea: Suppose I am trying to build a model that predicts the price of a house. Earlier in the course, we looked at this problem using tabulated data and a variety of different regression models.\n",
    "\n",
    "![](regression.png)\n",
    "\n",
    "In this example, the regression model could be any of a number of different models, such as linear regression, random forest, boosted trees, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that,instead of having a table of house features as our `X` matrix, we have image data. We could use a CNN model to make our predictor:\n",
    "\n",
    "![](cnn.png)\n",
    "\n",
    "Here, the CNN can be as simple or as complicated as we like, but the general structure is to use Convo2D and pooling to decompose the image into a set of features. These features are then flattened to form a vector which more or less summarizes the salient features of the house. These features are then the inputs to dense layers which feed an output layer which provides our final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we now want to create a more advanced model that combines these two different types of data, tabular data and image data to make better predictions.\n",
    "\n",
    "It's possible (but not advisable) to simply append the tabulated data values to the image data. Here the numeric values from the data table are treated as extra pixels in the image.\n",
    "\n",
    "![](simple_hybrid.png)\n",
    "\n",
    "I don't think this is a good approach because the nature of the convolution allows pixels to shift left and right (and up and down). This would essentially scramble the meaning of these features. I.e, a house with 4 bedrooms, 3 bathrooms and 1200 square feet would be treated the same as a house with 3 bedrooms and 1200 bathrooms, which is not going to be very sensible. We need to ensure we are using convolution on the image, but not on the tabulated features.  \n",
    "\n",
    "There are a number of possible approaches to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest approach might be to make a meta model which takes as its inputs the predictions from the two models we have already built.\n",
    "![](meta_model1.png)\n",
    "\n",
    "The two separate sets of results could be combined into a single data frame using code such as:\n",
    "\n",
    "```\n",
    "hybrid_df['prediction_1'] = regression_model.predict(X)\n",
    "hybrid_df['prediction_2'] = CNN_model.predict(image)\n",
    "\n",
    "meta_model.predict(hybrid_df)\n",
    "```\n",
    "\n",
    "Bearing in mind, of course, to maintain correspondence between the two prediction models so that results from one house's features are not aligned with results from another house's image.\n",
    "\n",
    "\n",
    "While this model has the advantage of combing information both tabular and image, with appropriate treatement for each type, it is somewhat limited in how it can combine that data. The final prediction might not be any more complex than a simple weighted average between the two predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than combine the data at the output of the initial predictors, another approach would be to treate the prediction from the CNN model as an additional feature of the tabulated data.\n",
    "\n",
    "![](meta_model2.png)\n",
    "\n",
    "Rather than create a new data frame from the initial predictions, we append the CNN prediction using code such as:\n",
    "\n",
    "```\n",
    "df['CNN_prediction'] = CNN_model.predict(image)\n",
    "\n",
    "meta_model.predict(df)\n",
    "```\n",
    "\n",
    "or:\n",
    "\n",
    "```\n",
    "meta_model.predict(pd.concat([df, CNN_model.predict(image)], axis = 1]))\n",
    "```\n",
    "\n",
    "This approach allows for greater interaction between the features than simply looking at the final predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps a drawback of these last two approaches is that they over-simplify the information from the image. Here, the entire image has been simplified down to just a single value (a prediction of price.) Rather than just use this one value as the only output from the CNN model, we could get a richer set of features derived from the image. Fortunately, the CNN allows us to get these values from one of the flat layers prior to the final output.\n",
    "\n",
    "![](meta_model3.png)\n",
    "\n",
    "The additional features, `CNN_featurex` don't have an interpretation as simple as the original features (bedrooms, bathrooms, square feet), but they are features nonetheless which may be informative for our final model. They can be thought of as digested components of the image.\n",
    "\n",
    "Note that we are not using the predicted output from the CNN model to make our ultimate prediction (although we could, if we wanted, add that predicted value in as yet another feature column). Nevertheless, we need to train our CNN model to some target, so the predicted price from the CNN is needed at during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach would be to replace the CNN prediction model with an autoencoder trained to decompose and then recompose images. The interior of the autoencoder (bottleneck) is a vector of features which describes the house and can be used either to reconstruct an image of the house, or to feed a final model.\n",
    "\n",
    "![](meta_model4.png)\n",
    "\n",
    "Here the image provides the target needed to train the autoencoder and the bottleneck layer provides the vector of encoded image features which are appeneded to the orginal set of features to make the final prediction.\n",
    "\n",
    "As we saw in the autoencoder sprint, tensorflow allows models to be built from other models, just as from layers, so this gives us the ability to break the autoencoder into multiple parts and use the encoder portion both to feed the decoder and and to generate its own output vector.\n",
    "\n",
    "The keras Functional API  `layers.concatenate()` method allows us to join two different vectors inside a keras model. So this process can be done either with a keras model into a pandas data frame, into an sklearn model, or entirely inside of a keras model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
